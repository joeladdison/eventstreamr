The video encoding pipeline
===========================

As soon as videos have been recorded, the encoding process can begin, which operates roughly as follows:

1. Raw video (including audio) of the talk is stored, as dv files, in a predetermined recordings directory. Each talk will normally be divided into multiple files which will need to be stitched together for the final video.
2. At the end of the session or day, someone (senior and sensible) from the AV team uses the queue manager to run through the talks that have been recorded so far, which adds a json file containing information required to encode that talk.
3. At some later point the video encoder is passed the json information for that talk, and uses it to encode the video into the final formats (mp4, ogv, ogg).
4. The final videos are uploaded to various places on the internet.

The pipeline makes a number of assumptions:

- Each talk is bookended by a title slide containing the title and presenters of the talk, and a credits slide with user-editable text content. The backgrounds of these slides are static images.
- Every talk is in the schedule, and the recordings matching it start within 10 minutes either side of the scheduled duration. *(This is probably not a very good assumption.)*
- Once the talk has started, all video content from the start time to the end time should be encoded. There are no cases where content needs to be cut from the middle of the talk. *(This may also not always be a good assumption.)*

These assumptions should hold for the majority of talks, though. Special cases can be dealt with manually as required.


dependencies
------------

Mainly moviepy, which has a bunch of dependencies of its own. ImageMagick is an optional dependency for moviepy but it's required for the title/credits slides.


setup
-----

The queue manager, video encoding script  and youtube uploader all refer to a single encoding config file, located by default at `/path/to/eventstreamr/encoding/config.json`. This file must have the following format (defaults are as follows):

    {
        "dirs": {
            "queue": "test/queue/",
            "recordings": "/localbackup/av/",
            "output": "test/output/"
        },
        "schedule": "test/schedule.json",
        "backgrounds": {
            "title": "media/title.jpg",
            "credits": "media/credits.jpg"
        }
    }

Relative paths are taken relative to the `/path/to/eventstreamr/encoding/` directory. Add or leave off trailing slashes as you wish.

The schedule-matching parts of the code assume the schedule data is in the format generated by the pycon-au.org site: `http://2015.pycon-au.org/programme/schedule/json`.

The background images should be static images that contain everything required for the title/credits slides except the dynamic text - so include any sponsor or conference logos in the image itself, since they will not be added. If you're going to change these, you'll probably also need to change the hardcoded text locations in the code, since they're not configurable otherwise and will vary for each image.


queue manager
-------------

Run `queue_manager.py`. You'll be given a list of talks for which possible matching recordings have been found. Enter the schedule id for the talk you wish to process. A `vlc` window will pop up with a playlist of each of the suggested matching files - note the filename and offset for the start and end files, and enter the details when prompted.

For both the start and end offsets, enter the time in minutes and seconds (e.g. `01:56`). The final video will begin from the start time in the start file, continue through all intermediate  files, and finish at the end time in the end file.

After you've finished with one talk, a json file containing the required information for encoding will be created in the queue directory you set in `config.json`. The filename will be based on the schedule id of the talk (e.g. `test/queue/39.json`).

Some people dream of eventually making this a web app.


video encoder
-------------

The next stage involves taking the talk config generated by the queue manager, and encoding the final video into the desired formats. This is done by the `encode_video.py` script, which takes in the talk config, in json format, as a string passed in as a command line argument, since Ryan wants it that way. To test this component on its own, something like this works:

    python encode_video.py -s "`cat test/queue/39.json`"

The final videos will be saved to the output directory specified in `config.json`.

youtube uploader
----------------

I'm not sure what's involved with this, but this was in the previous README:

Requires a yaml config file with your develop api details, located at ~/.googleapi.yml

    client_id: client_id_here
    client_secret: client_secret_here
~                                                                               
See `youtube_uploader.py`.
~                                                                               
~                                                                               
~                                                                               
~                                                                               
~                                                                               
~                                                                               
~                                                                               
~                                                                               
~                             
